{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe56e075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries yang diperlukan\n",
    "!pip install pandas numpy scikit-learn nltk gensim wordcloud matplotlib seaborn pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49712d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Topic Modeling\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbed1aab",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "Upload file CSV Anda ke Colab menggunakan file upload atau dari Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931c8602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baca file CSV yang sudah diupload ke Colab\n",
    "# Upload file CSV Anda ke file panel Colab (klik ikon folder di kiri > upload)\n",
    "# Kemudian sesuaikan nama file di bawah ini\n",
    "\n",
    "# Ganti 'nama_file.csv' dengan nama file CSV Anda\n",
    "df = pd.read_csv('itb_news.csv')  # Sesuaikan nama file\n",
    "\n",
    "# ATAU jika file ada di folder tertentu:\n",
    "# df = pd.read_csv('folder/nama_file.csv')\n",
    "\n",
    "# ATAU jika file di Google Drive:\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# df = pd.read_csv('/content/drive/MyDrive/path/to/your/file.csv')\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Column names: {df.columns.tolist()}\")\n",
    "\n",
    "# Pastikan ada kolom yang berisi teks berita\n",
    "# Jika nama kolom berbeda dari 'text', sesuaikan di bawah ini:\n",
    "# Contoh: jika kolom berita bernama 'content' atau 'artikel':\n",
    "# df = df.rename(columns={'content': 'text'})\n",
    "\n",
    "# Atau jika ingin menggabungkan beberapa kolom:\n",
    "# df['text'] = df['judul'].fillna('') + ' ' + df['konten'].fillna('')\n",
    "\n",
    "print(f\"\\nTotal documents: {len(df)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9057db6",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a997493",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self, language='english'):\n",
    "        self.stop_words = set(stopwords.words(language))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Membersihkan teks dari karakter khusus\"\"\"\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    def tokenize_and_lemmatize(self, text):\n",
    "        \"\"\"Tokenisasi dan lemmatisasi\"\"\"\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [self.lemmatizer.lemmatize(word) for word in tokens \n",
    "                 if word not in self.stop_words and len(word) > 2]\n",
    "        return tokens\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Pipeline preprocessing lengkap\"\"\"\n",
    "        cleaned = self.clean_text(text)\n",
    "        tokens = self.tokenize_and_lemmatize(cleaned)\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "# Preprocessing\n",
    "preprocessor = TextPreprocessor()\n",
    "df['cleaned_text'] = df['text'].apply(preprocessor.preprocess)\n",
    "df['tokens'] = df['cleaned_text'].apply(lambda x: x.split())\n",
    "\n",
    "print(\"\\nContoh hasil preprocessing:\")\n",
    "print(f\"Original: {df['text'][0]}\")\n",
    "print(f\"Cleaned: {df['cleaned_text'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde5d167",
   "metadata": {},
   "source": [
    "## 3. Topic Modeling dengan Scikit-learn LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfda88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization\n",
    "n_topics = 3  # Sesuaikan dengan kebutuhan\n",
    "vectorizer = CountVectorizer(max_features=1000, min_df=2, max_df=0.8)\n",
    "doc_term_matrix = vectorizer.fit_transform(df['cleaned_text'])\n",
    "\n",
    "print(f\"Document-Term Matrix shape: {doc_term_matrix.shape}\")\n",
    "\n",
    "# Train LDA model\n",
    "lda_model = LatentDirichletAllocation(\n",
    "    n_components=n_topics,\n",
    "    random_state=42,\n",
    "    max_iter=20,\n",
    "    learning_method='online'\n",
    ")\n",
    "\n",
    "lda_output = lda_model.fit_transform(doc_term_matrix)\n",
    "\n",
    "print(f\"\\nModel trained with {n_topics} topics\")\n",
    "print(f\"Log Likelihood: {lda_model.score(doc_term_matrix)}\")\n",
    "print(f\"Perplexity: {lda_model.perplexity(doc_term_matrix)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1528f3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top words per topic\n",
    "def display_topics(model, feature_names, n_top_words=10):\n",
    "    topics = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_indices = topic.argsort()[-n_top_words:][::-1]\n",
    "        top_words = [feature_names[i] for i in top_indices]\n",
    "        topics[f\"Topic {topic_idx + 1}\"] = top_words\n",
    "        print(f\"\\nTopic {topic_idx + 1}:\")\n",
    "        print(\", \".join(top_words))\n",
    "    return topics\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "topics_dict = display_topics(lda_model, feature_names, n_top_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939b117e",
   "metadata": {},
   "source": [
    "## 4. Topic Modeling dengan Gensim (Alternatif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d16e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat dictionary dan corpus untuk Gensim\n",
    "dictionary = corpora.Dictionary(df['tokens'])\n",
    "corpus = [dictionary.doc2bow(text) for text in df['tokens']]\n",
    "\n",
    "# Train Gensim LDA model\n",
    "gensim_lda = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=n_topics,\n",
    "    random_state=42,\n",
    "    passes=10,\n",
    "    alpha='auto',\n",
    "    per_word_topics=True\n",
    ")\n",
    "\n",
    "# Display topics\n",
    "print(\"\\nGensim LDA Topics:\")\n",
    "for idx, topic in gensim_lda.print_topics(-1):\n",
    "    print(f\"Topic {idx + 1}: {topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070dfe10",
   "metadata": {},
   "source": [
    "## 5. Visualisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7289290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document-Topic Distribution\n",
    "df['dominant_topic'] = lda_output.argmax(axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "topic_counts = df['dominant_topic'].value_counts().sort_index()\n",
    "sns.barplot(x=topic_counts.index, y=topic_counts.values)\n",
    "plt.title('Distribution of Documents Across Topics')\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('Number of Documents')\n",
    "plt.xticks(range(n_topics), [f'Topic {i+1}' for i in range(n_topics)])\n",
    "plt.tight_layout()\n",
    "plt.savefig('topic_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed06998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Cloud untuk setiap topic\n",
    "fig, axes = plt.subplots(1, n_topics, figsize=(15, 5))\n",
    "if n_topics == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    top_indices = topic.argsort()[-20:][::-1]\n",
    "    top_words = {feature_names[i]: topic[i] for i in top_indices}\n",
    "    \n",
    "    wordcloud = WordCloud(\n",
    "        width=400, \n",
    "        height=300, \n",
    "        background_color='white'\n",
    "    ).generate_from_frequencies(top_words)\n",
    "    \n",
    "    axes[topic_idx].imshow(wordcloud, interpolation='bilinear')\n",
    "    axes[topic_idx].set_title(f'Topic {topic_idx + 1}', fontsize=14)\n",
    "    axes[topic_idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('wordclouds.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354684ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyLDAvis - Visualisasi interaktif (Gensim)\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(gensim_lda, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f7d400",
   "metadata": {},
   "source": [
    "## 6. Save Model dan Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf91a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save semua model dan preprocessor\n",
    "import joblib\n",
    "\n",
    "# Save sklearn LDA model\n",
    "joblib.dump(lda_model, 'lda_model.pkl')\n",
    "joblib.dump(vectorizer, 'vectorizer.pkl')\n",
    "joblib.dump(preprocessor, 'preprocessor.pkl')\n",
    "\n",
    "# Save Gensim model\n",
    "gensim_lda.save('gensim_lda_model')\n",
    "dictionary.save('dictionary.pkl')\n",
    "\n",
    "# Save topics dictionary\n",
    "with open('topics.pkl', 'wb') as f:\n",
    "    pickle.dump(topics_dict, f)\n",
    "\n",
    "# Save processed data\n",
    "df.to_csv('processed_data.csv', index=False)\n",
    "\n",
    "print(\"\\nModel dan artifacts berhasil disimpan!\")\n",
    "print(\"Files:\")\n",
    "print(\"- lda_model.pkl\")\n",
    "print(\"- vectorizer.pkl\")\n",
    "print(\"- preprocessor.pkl\")\n",
    "print(\"- gensim_lda_model\")\n",
    "print(\"- dictionary.pkl\")\n",
    "print(\"- topics.pkl\")\n",
    "print(\"- processed_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9421dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files untuk deploy ke Streamlit\n",
    "from google.colab import files\n",
    "\n",
    "files.download('lda_model.pkl')\n",
    "files.download('vectorizer.pkl')\n",
    "files.download('preprocessor.pkl')\n",
    "files.download('topics.pkl')\n",
    "files.download('processed_data.csv')\n",
    "files.download('topic_distribution.png')\n",
    "files.download('wordclouds.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f52ac4",
   "metadata": {},
   "source": [
    "## 7. Test Prediction pada Teks Baru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b90724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_topic(text, preprocessor, vectorizer, model, n_words=5):\n",
    "    \"\"\"Prediksi topik untuk teks baru\"\"\"\n",
    "    # Preprocess\n",
    "    cleaned = preprocessor.preprocess(text)\n",
    "    \n",
    "    # Transform\n",
    "    vectorized = vectorizer.transform([cleaned])\n",
    "    \n",
    "    # Predict\n",
    "    topic_dist = model.transform(vectorized)[0]\n",
    "    dominant_topic = topic_dist.argmax()\n",
    "    \n",
    "    # Get top words\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    topic = model.components_[dominant_topic]\n",
    "    top_indices = topic.argsort()[-n_words:][::-1]\n",
    "    top_words = [feature_names[i] for i in top_indices]\n",
    "    \n",
    "    return {\n",
    "        'dominant_topic': int(dominant_topic + 1),\n",
    "        'topic_distribution': topic_dist.tolist(),\n",
    "        'top_words': top_words,\n",
    "        'confidence': float(topic_dist[dominant_topic])\n",
    "    }\n",
    "\n",
    "# Test\n",
    "test_text = \"Python is great for building machine learning models and data analysis.\"\n",
    "result = predict_topic(test_text, preprocessor, vectorizer, lda_model)\n",
    "\n",
    "print(f\"\\nTest Text: {test_text}\")\n",
    "print(f\"\\nPredicted Topic: Topic {result['dominant_topic']}\")\n",
    "print(f\"Confidence: {result['confidence']:.2%}\")\n",
    "print(f\"Top Words: {', '.join(result['top_words'])}\")\n",
    "print(f\"\\nTopic Distribution:\")\n",
    "for i, prob in enumerate(result['topic_distribution']):\n",
    "    print(f\"  Topic {i+1}: {prob:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
